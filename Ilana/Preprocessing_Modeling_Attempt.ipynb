{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/IlanaWeinstein/Desktop/Machine-Learning-Project/Data'\n",
    "train = pd.DataFrame(pd.read_csv(data_path + '/' + 'train.csv', sep=','))\n",
    "dev = pd.DataFrame(pd.read_csv(data_path + '/' + 'dev.csv', sep=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['label'], axis=1)\n",
    "y_train = train['label']\n",
    "\n",
    "X_val = dev.drop(['label'], axis=1)\n",
    "y_val = dev['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = X_train['review'], y_train\n",
    "val_texts, val_labels     = X_val['review'], y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In the lab we use built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
    "**In this HW, you are required to implement the `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
    "\n",
    "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
    "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
    "\n",
    "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \n",
    "    preprocessed_data = []\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = en_core_web_sm.load()\n",
    "    \n",
    "    #new to spacy, decided to not change any settings\n",
    "    for text in tqdm(data):\n",
    "        doc = nlp(text)\n",
    "        new_list = []\n",
    "        for token in doc:\n",
    "            new_list.append(token.text)\n",
    "        preprocessed_data.append(new_list)\n",
    "    \n",
    "\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train = train_texts[:int(.3 * len(train_texts))]\n",
    "test_val = val_texts[:int(.3 * len(val_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75262/75262 [32:36<00:00, 38.47it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10775/10775 [04:40<00:00, 38.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# takes  while to run \n",
    "\n",
    "# train_data = preprocess_data(train_texts)\n",
    "# print('Done train')\n",
    "# val_data = preprocess_data(val_texts)\n",
    "\n",
    "train_data = preprocess_data(test_train)\n",
    "print('Done train')\n",
    "val_data = preprocess_data(test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    def __init__(self, max_features):\n",
    "        self.max_features = max_features\n",
    "        self.vocab_list = None\n",
    "        self.token_to_index = None\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
    "        \n",
    "        vocab_master = np.array([ elem for row in dataset for elem in row])\n",
    "        \n",
    "        word_list,count = np.unique(vocab_master,return_counts = True)\n",
    "        word_list_sorted = word_list[np.argsort(-count)]\n",
    "        \n",
    "        self.vocab_list = word_list_sorted[:self.max_features]\n",
    "\n",
    "        # Create a token indexer, self.token_to_index, that will return index of the token in self.vocab_list\n",
    "        self.token_to_index = {}\n",
    "\n",
    "        for i,word in enumerate(self.vocab_list):\n",
    "            self.token_to_index[word] = i\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        # This function transforms text dataset into a matrix, data_matrix\n",
    "        \"\"\"\n",
    "        YOUR CODE GOES HERE\n",
    "        \"\"\"\n",
    "        #dictionary, append count of words only in that row\n",
    "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
    "        for i,row in enumerate(dataset):\n",
    "            for word in row:\n",
    "                data_matrix[i][self.token_to_index.get(word)] += 1\n",
    "  \n",
    "        \n",
    "        return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000 # TODO: Replace None with a number\n",
    "vectorizer = Vectorizer(max_features=max_features)\n",
    "vectorizer.fit(train_data)\n",
    "X_train = vectorizer.transform(train_data)\n",
    "X_val = vectorizer.transform(val_data)\n",
    "# X_test = vectorizer.transform(test_data)\n",
    "\n",
    "y_train = np.array(train_labels[:75262]) #remove slice with full dataset \n",
    "y_val = np.array(val_labels[:10775])\n",
    "# y_test = np.array(test_labels)\n",
    "\n",
    "vocab = vectorizer.vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  4.,  2., ...,  2.,  2.,  2.],\n",
       "       [ 9.,  7.,  9., ...,  5.,  5.,  5.],\n",
       "       [ 4.,  3.,  3., ...,  1.,  1.,  1.],\n",
       "       ...,\n",
       "       [11., 10.,  8., ...,  6.,  6.,  6.],\n",
       "       [16.,  8.,  8., ...,  8.,  8.,  8.],\n",
       "       [11.,  9.,  6., ...,  6.,  6.,  6.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', 'the', ',', ..., '[', 'frozen', 'Being'], dtype='<U227')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We train logistic regression model and save prediction for train, val and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define Logistic Regression model\n",
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# Fit the model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction using the trained model\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "# y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.901, F1 score: 0.051\n",
      "Validation accuracy: 0.899, F1 score: 0.036\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
    "# print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
    "#       f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
